{
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q57Zf8p9ivZa",
        "J5d58CfLoWZj",
        "zdcR5JHdp6qY"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b467083e9b8e48a28fb390e08d7f2f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3504bd3fded42feac88374368a90820",
              "IPY_MODEL_5d7e45d7faa74b559b8e7abd68fef7ab",
              "IPY_MODEL_9fe8ebab28d0456d8345b6f0dbdef03a"
            ],
            "layout": "IPY_MODEL_be0f177a0db04aeb988c22588c3e95ef"
          }
        },
        "e3504bd3fded42feac88374368a90820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04362094140f43cdac4bfb163aeca3b5",
            "placeholder": "​",
            "style": "IPY_MODEL_7a4b8ea2a13443dc8775ab76b9785716",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5d7e45d7faa74b559b8e7abd68fef7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe372d22b3ab46138e676da79a593da3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ba45990a8124412b8bae4d5dca59130",
            "value": 2
          }
        },
        "9fe8ebab28d0456d8345b6f0dbdef03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91251c4ec6564aa7ab1b3536c5165840",
            "placeholder": "​",
            "style": "IPY_MODEL_72e00893957e4aa6a9aa33469d8b7f29",
            "value": " 2/2 [00:01&lt;00:00,  1.71s/it]"
          }
        },
        "be0f177a0db04aeb988c22588c3e95ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04362094140f43cdac4bfb163aeca3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4b8ea2a13443dc8775ab76b9785716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe372d22b3ab46138e676da79a593da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba45990a8124412b8bae4d5dca59130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91251c4ec6564aa7ab1b3536c5165840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e00893957e4aa6a9aa33469d8b7f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "q57Zf8p9ivZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "BNrsPceayp4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59801e7f-1833-47c5-870f-a39c738dc7e7",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:01.025595Z",
          "iopub.execute_input": "2024-07-08T07:47:01.025964Z",
          "iopub.status.idle": "2024-07-08T07:47:29.548179Z",
          "shell.execute_reply.started": "2024-07-08T07:47:01.025934Z",
          "shell.execute_reply": "2024-07-08T07:47:29.546892Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "bPez_ItJGWOa",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:29.551263Z",
          "iopub.execute_input": "2024-07-08T07:47:29.551548Z",
          "iopub.status.idle": "2024-07-08T07:47:42.023495Z",
          "shell.execute_reply.started": "2024-07-08T07:47:29.551524Z",
          "shell.execute_reply": "2024-07-08T07:47:42.022550Z"
        },
        "trusted": true,
        "outputId": "a85d1dd0-6734-4d05-e1e5-9c9722f93607"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "8vwn6xGIi03f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "jw-5oFU3ysk9",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:42.024785Z",
          "iopub.execute_input": "2024-07-08T07:47:42.025061Z",
          "iopub.status.idle": "2024-07-08T07:48:01.873411Z",
          "shell.execute_reply.started": "2024-07-08T07:47:42.025037Z",
          "shell.execute_reply": "2024-07-08T07:48:01.872639Z"
        },
        "trusted": true,
        "outputId": "0c01e669-21d5-400d-cd20-89b5258e0231"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-08 07:47:49.926511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 07:47:49.926646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 07:47:50.061199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:01.875808Z",
          "iopub.execute_input": "2024-07-08T07:48:01.876270Z",
          "iopub.status.idle": "2024-07-08T07:48:01.883024Z",
          "shell.execute_reply.started": "2024-07-08T07:48:01.876238Z",
          "shell.execute_reply": "2024-07-08T07:48:01.882186Z"
        },
        "trusted": true,
        "id": "etdQqMFt-YHG",
        "outputId": "b26c402a-2143-49d7-f464-9002bbf3e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**In case of Llama 2, the following prompt template is used for the chat models**"
      ],
      "metadata": {
        "id": "J5d58CfLoWZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "System Prompt (optional) to guide the model\n",
        "\n",
        "\n",
        "User prompt (required) to give the instruction\n",
        "\n",
        "\n",
        "Model Answer (required)"
      ],
      "metadata": {
        "id": "5lYYzhCRov5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "AS1Ee8JunXgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**We will reformat our instruction dataset to follow Llama 2 template.**\n",
        "\n",
        "Complete Reformat Dataset following the Llama 2 template"
      ],
      "metadata": {
        "id": "ck708D51o-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: You don’t need to follow a specific prompt template if you’re using the base Llama 2 model instead of the chat version."
      ],
      "metadata": {
        "id": "jSFfMkSpphFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset1 = load_dataset(\"poornima9348/finance-alpaca-1k-test\")\n",
        "dataset2 = load_dataset(\"ssbuild/alpaca_finance_en\")"
      ],
      "metadata": {
        "id": "_4_4BwpuiJEH",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:01.884072Z",
          "iopub.execute_input": "2024-07-08T07:48:01.884337Z",
          "iopub.status.idle": "2024-07-08T07:48:06.144535Z",
          "shell.execute_reply.started": "2024-07-08T07:48:01.884315Z",
          "shell.execute_reply": "2024-07-08T07:48:06.143781Z"
        },
        "trusted": true,
        "outputId": "fa840d2f-8a24-4a12-d4a4-7df28a2a3357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/121 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d5c1c45783a4af5b1a0252237906230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a78d6a2a0f446fc99dc4931605ce153"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80ef02c119944baa90fb6d1830fc17f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef230220a546429c8db3a50b673a02dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/23.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4cff120c064117ac05cd0073aa0fe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24df77b3eb314c4db6010e8336ac8546"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1"
      ],
      "metadata": {
        "id": "zno1pBMwiMLn",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.145592Z",
          "iopub.execute_input": "2024-07-08T07:48:06.145891Z",
          "iopub.status.idle": "2024-07-08T07:48:06.151619Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.145864Z",
          "shell.execute_reply": "2024-07-08T07:48:06.150730Z"
        },
        "trusted": true,
        "outputId": "a4c4e9ca-1834-4a76-8a20-3a32f22877dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2"
      ],
      "metadata": {
        "id": "twCHMEZtiPZQ",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.152744Z",
          "iopub.execute_input": "2024-07-08T07:48:06.153039Z",
          "iopub.status.idle": "2024-07-08T07:48:06.243255Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.153016Z",
          "shell.execute_reply": "2024-07-08T07:48:06.242379Z"
        },
        "trusted": true,
        "outputId": "e026319b-b583-4ca6-fc57-93a2178a831d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'instruction', 'input', 'output'],\n        num_rows: 68912\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 'instruction' and 'output' columns into a new 'text' column\n",
        "def combine_text_columns(example):\n",
        "    return {'text': f\"{example['instruction']} ### {example['output']}\"}\n",
        "\n",
        "# Apply the function to each example in the dataset\n",
        "dataset1 = dataset1.map(combine_text_columns)\n",
        "dataset2 = dataset2.map(combine_text_columns)\n",
        "\n",
        "# Remove 'instruction', 'input' and 'output' columns\n",
        "dataset1['test']=dataset1['test'].remove_columns(['instruction','input', 'output'])\n",
        "dataset2['train']=dataset2['train'].remove_columns(['instruction','input', 'output','id'])"
      ],
      "metadata": {
        "id": "_ecqAmGv7Bjm",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.244568Z",
          "iopub.execute_input": "2024-07-08T07:48:06.244902Z",
          "iopub.status.idle": "2024-07-08T07:48:12.404242Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.244873Z",
          "shell.execute_reply": "2024-07-08T07:48:12.403369Z"
        },
        "trusted": true,
        "outputId": "b40c3960-5b2c-44ca-aaf8-96f728c68508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e07119cbf9544d798f43b3be274a3f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "033c965d05ea49d8bf246afaa8b8215f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split on the necessary dataset if required\n",
        "split_dataset1 = dataset1['test'].train_test_split(train_size=0.8)\n",
        "split_dataset2 = dataset2['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "8Ceyq0vihS6I",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.405548Z",
          "iopub.execute_input": "2024-07-08T07:48:12.405908Z",
          "iopub.status.idle": "2024-07-08T07:48:12.591503Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.405876Z",
          "shell.execute_reply": "2024-07-08T07:48:12.590538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset1"
      ],
      "metadata": {
        "id": "h0g-eEQhhcS1",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.594356Z",
          "iopub.execute_input": "2024-07-08T07:48:12.594640Z",
          "iopub.status.idle": "2024-07-08T07:48:12.676255Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.594616Z",
          "shell.execute_reply": "2024-07-08T07:48:12.675290Z"
        },
        "trusted": true,
        "outputId": "13940e5e-9e2f-4078-c5f0-5ef8eaf65d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 200\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset2"
      ],
      "metadata": {
        "id": "b-CJCK3IikOm",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.677465Z",
          "iopub.execute_input": "2024-07-08T07:48:12.677852Z",
          "iopub.status.idle": "2024-07-08T07:48:12.685802Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.677803Z",
          "shell.execute_reply": "2024-07-08T07:48:12.684780Z"
        },
        "trusted": true,
        "outputId": "a7b2937e-2a3d-4ef0-b7f8-25fdc31e69aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55129\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13783\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the datasets\n",
        "merged_train = concatenate_datasets([split_dataset1['train'], split_dataset2['train']])\n",
        "merged_test = concatenate_datasets([split_dataset1['test'], split_dataset2['test']])\n",
        "\n",
        "# Create a new DatasetDict with the merged datasets\n",
        "merged_dataset = DatasetDict({\n",
        "    'train': merged_train,\n",
        "    'test': merged_test\n",
        "})\n",
        "\n",
        "# Filter out None values in case some splits are missing\n",
        "merged_dataset = DatasetDict({k: v for k, v in merged_dataset.items() if v is not None})\n",
        "\n",
        "# Print the merged dataset to verify\n",
        "print(merged_dataset)"
      ],
      "metadata": {
        "id": "Azyahwk_ir69",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.687038Z",
          "iopub.execute_input": "2024-07-08T07:48:12.687786Z",
          "iopub.status.idle": "2024-07-08T07:48:12.702890Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.687760Z",
          "shell.execute_reply": "2024-07-08T07:48:12.701884Z"
        },
        "trusted": true,
        "outputId": "27bb5735-dc2f-4a16-fabe-cc998e56d2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55929\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13983\n    })\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_train_dataset = merged_dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_dataset = merged_train_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "id": "NT6hchLYdTmS",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.703904Z",
          "iopub.execute_input": "2024-07-08T07:48:12.704187Z",
          "iopub.status.idle": "2024-07-08T07:48:13.225899Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.704165Z",
          "shell.execute_reply": "2024-07-08T07:48:13.224997Z"
        },
        "trusted": true,
        "outputId": "94e6f08e-0464-44c9-e4b4-e2250609350b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd808070d3a4197ac02b145b4b81921"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_test_dataset = merged_dataset['test'].shuffle(seed=42).select(range(100))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_test_dataset = merged_test_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "id": "F6QT7Kv4w7wG",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.227016Z",
          "iopub.execute_input": "2024-07-08T07:48:13.227289Z",
          "iopub.status.idle": "2024-07-08T07:48:13.281438Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.227265Z",
          "shell.execute_reply": "2024-07-08T07:48:13.280585Z"
        },
        "trusted": true,
        "outputId": "9dbe2704-13b7-4f11-cd92-12063eec228e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fcfae05100a49d59a544780110a9b18"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_dataset['text'][0]"
      ],
      "metadata": {
        "id": "acgKXbKWxMXH",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.282512Z",
          "iopub.execute_input": "2024-07-08T07:48:13.282793Z",
          "iopub.status.idle": "2024-07-08T07:48:13.293796Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.282769Z",
          "shell.execute_reply": "2024-07-08T07:48:13.292887Z"
        },
        "trusted": true,
        "outputId": "d9920e5e-b6b1-4d04-8130-37ac2558699e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'In USA, what circumstances (if any) make it illegal for a homeless person to “rent” an address? ### It depends on the rules in the specific places you stay.  Specific places being countries or states.   Some states may consider pension payments to be taxable income, others may not.  Some may consider presence for X days to constitute residency, X days may be 60 days in a calendar year whether or not those days are continuous.   It doesn\\'t matter so much where your mailbox or mail handling service is located, it matters: You may owe taxes in more than one place.  Some states will allow you to offset other states\\' taxes against theirs.  Some states in the US are really harsh on income taxes.  It\\'s my understanding that if you own real estate in New York, all of your income, no matter the source, is taxable income in New York whether or not you were ever in the state that year. Ultimately, you can\\'t just put up your hand and say, \"that\\'s my tax domicile so I\\'m exempt from all your taxes.\"  There is no umbrella US regulation on this topic, the states determine who they consider to be residents and how those residents are to be taxed. While it\\'s possible you may be considered a resident of multiple states and owe income taxes in multiple states, it\\'s equally possible that you won\\'t meet the residency criteria for any state regardless of whether or not that state has an income tax.  The issue you face, as addressed in @Jay\\'s answer, Oklahoma will consider you a resident of OK until you have established residency somewhere else.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**How to fine tune Llama 2**"
      ],
      "metadata": {
        "id": "zdcR5JHdp6qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Free Google Colab offers a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)"
      ],
      "metadata": {
        "id": "FDEaxMxFqAV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We also need to consider the overhead due to optimizer states, gradients, and forward activations"
      ],
      "metadata": {
        "id": "rwJnbDU1qNx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA."
      ],
      "metadata": {
        "id": "DIeTamzXqcC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here."
      ],
      "metadata": {
        "id": "1lDdMCfyqvy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3** **Define the LLM llama-2 model and QLoRa parameters**"
      ],
      "metadata": {
        "id": "8gXkTpnRq9nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a llama-2-7b-chat-hf model (chat model)\n",
        "2. Train it on the our custom dataset from hugging face (1,000 samples), which will produce our fine-tuned model Llama-2-7b-chat-finetune"
      ],
      "metadata": {
        "id": "A3XhjPfHq_mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch"
      ],
      "metadata": {
        "id": "majOmi5T77D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-finance-chatbot-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 350\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "MFdhJds4yzuh",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.295083Z",
          "iopub.execute_input": "2024-07-08T07:48:13.295345Z",
          "iopub.status.idle": "2024-07-08T07:48:13.304394Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.295323Z",
          "shell.execute_reply": "2024-07-08T07:48:13.303526Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4:Load everything and start the fine-tuning process**"
      ],
      "metadata": {
        "id": "G9w5Txi5wI2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
        "\n",
        "\n",
        "2. Then, we’re configuring bitsandbytes for 4-bit quantization.\n",
        "\n",
        "\n",
        "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
        "\n",
        "\n",
        "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!"
      ],
      "metadata": {
        "id": "qNx4Es23wjzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Reformatted dataset\n",
        "dataset = transformed_dataset\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zqiIFRObOjgh",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.305806Z",
          "iopub.execute_input": "2024-07-08T07:48:13.306167Z",
          "iopub.status.idle": "2024-07-08T08:38:28.648781Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.306136Z",
          "shell.execute_reply": "2024-07-08T08:38:28.647872Z"
        },
        "trusted": true,
        "outputId": "504010eb-13ad-4968-8d50-7f2b5ebdbe5b",
        "colab": {
          "referenced_widgets": [
            "cdb4df6ae92148bc90c03ca9c649b4a1",
            "57fc82bd2a5b477eb727296252c64db5",
            "b71f35e7c7184863bd08c173917fc2cf",
            "bf667e14cecd4cc796f5eb79e6b09f3c",
            "84e9c98590a345569ab971a01a0e2830",
            "53e13b5cc8e14791a03804c5f3d43b81",
            "d39b2246aa7343f29ccb5d0e96014aad",
            "e416b097b01f4ee8bbbd32cd69a62616",
            "823dce8aeed14ea08559ab3b90f0a21f",
            "5eacc19acf76440c8e7b5e46b88ca5bf",
            "6d4036225c1040c880d42d47d9cb3211",
            "04cb610c1f6c454cbb19e2632930ee18",
            "02b694ad35ac43a1b6ff6ff30ef0e4e6"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb4df6ae92148bc90c03ca9c649b4a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57fc82bd2a5b477eb727296252c64db5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b71f35e7c7184863bd08c173917fc2cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf667e14cecd4cc796f5eb79e6b09f3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e9c98590a345569ab971a01a0e2830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53e13b5cc8e14791a03804c5f3d43b81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d39b2246aa7343f29ccb5d0e96014aad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e416b097b01f4ee8bbbd32cd69a62616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "823dce8aeed14ea08559ab3b90f0a21f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eacc19acf76440c8e7b5e46b88ca5bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d4036225c1040c880d42d47d9cb3211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04cb610c1f6c454cbb19e2632930ee18"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02b694ad35ac43a1b6ff6ff30ef0e4e6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 48:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.426200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.010200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.746500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.523300</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.795900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.531000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.810100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.453500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.782000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.516000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.710700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.460700</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.726000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.408600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.746100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.472400</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.678000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.422200</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.710600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.434900</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.758000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.404400</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.717600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.432500</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.627700</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=625, training_loss=1.6522029724121094, metrics={'train_runtime': 2917.1263, 'train_samples_per_second': 1.714, 'train_steps_per_second': 0.214, 'total_flos': 1.263688331624448e+16, 'train_loss': 1.6522029724121094, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "baYvk8rm92t-",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:42:25.992497Z",
          "iopub.execute_input": "2024-07-08T08:42:25.992906Z",
          "iopub.status.idle": "2024-07-08T08:42:26.288464Z",
          "shell.execute_reply.started": "2024-07-08T08:42:25.992874Z",
          "shell.execute_reply": "2024-07-08T08:42:26.287278Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Steps to save the trained fine tuned model and push to hugging face hub"
      ],
      "metadata": {
        "id": "7LD14q4rJ3-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#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"
      ],
      "metadata": {
        "trusted": true,
        "id": "V1YFB-y9-YHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents to ensure files are saved\n",
        "print(\"Contents of new_model directory:\", os.listdir(new_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIu1IjPpQpxi",
        "outputId": "86477673-5759-47ce-e536-72758f2ae6dc",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:42:47.338471Z",
          "iopub.execute_input": "2024-07-08T08:42:47.338821Z",
          "iopub.status.idle": "2024-07-08T08:42:47.344254Z",
          "shell.execute_reply.started": "2024-07-08T08:42:47.338792Z",
          "shell.execute_reply": "2024-07-08T08:42:47.343309Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['adapter_config.json', 'README.md', 'adapter_model.bin']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5:Use the text generation pipeline to ask questions. Note that I’m formatting the input to match Llama 2 prompt template.**"
      ],
      "metadata": {
        "id": "TDqxnjvExGG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt =\"Generate a title for a blog about the Nobel Prize ceremony.'\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "T8ZhDdD0-LpE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "84e28440-d8fe-4bde-fdb0-00747637f880",
        "execution": {
          "iopub.status.busy": "2024-07-08T03:10:20.432675Z",
          "iopub.execute_input": "2024-07-08T03:10:20.433779Z",
          "iopub.status.idle": "2024-07-08T03:10:41.972639Z",
          "shell.execute_reply.started": "2024-07-08T03:10:20.433733Z",
          "shell.execute_reply": "2024-07-08T03:10:41.971651Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<s>[INST] Generate a title for a blog about the Nobel Prize ceremony.' [/INST] The Nobel Prize Ceremony: A Celebration of Excellence.\n\nThe Nobel Prize ceremony is an annual event that recognizes the achievements of individuals who have made significant contributions to their respective fields. It is a celebration of excellence and a testament to the power of human ingenuity. The ceremony is a time for the recipients to be recognized for\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Check the plots on tensorboard, as follows**"
      ],
      "metadata": {
        "id": "NBtXo_Tgw43o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "crj9svNe4hU5",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 7: Store New Llama2 Model (Llama-2-7b-finance-chatbot-finetune)**"
      ],
      "metadata": {
        "id": "CwbthYYhxs8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything."
      ],
      "metadata": {
        "id": "ugs6EbD9xtAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define model_name and new_model\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "new_model = \"Llama-2-7b-finance-chatbot-finetune\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(new_model):\n",
        "    os.makedirs(new_model)\n",
        "\n",
        "# Define the offload directory\n",
        "offload_dir = \"/kaggle/working/\"\n",
        "\n",
        "# Ensure the offload directory exists\n",
        "if not os.path.exists(offload_dir):\n",
        "    os.makedirs(offload_dir)\n",
        "\n",
        "try:\n",
        "    # Reload model in FP16 and merge it with LoRA weights\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        low_cpu_mem_usage=True,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",  # Automatically splits the model across available GPUs\n",
        "        offload_folder=offload_dir  # Offload to the specified directory\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, new_model)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    # Reload tokenizer to save it\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(new_model)\n",
        "    tokenizer.save_pretrained(new_model)\n",
        "\n",
        "    # List the contents to ensure files are saved\n",
        "    print(\"Contents of new_model directory:\", os.listdir(new_model))\n",
        "\n",
        "    # Zip the new_model directory\n",
        "    #shutil.make_archive(new_model, 'zip', new_model)\n",
        "\n",
        "    # Download the zipped file\n",
        "    #files.download(new_model + \".zip\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"Out of memory error. Try using a smaller model or increasing GPU memory.\")\n",
        "        torch.cuda.empty_cache()\n",
        "    else:\n",
        "        raise e\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b467083e9b8e48a28fb390e08d7f2f03",
            "e3504bd3fded42feac88374368a90820",
            "5d7e45d7faa74b559b8e7abd68fef7ab",
            "9fe8ebab28d0456d8345b6f0dbdef03a",
            "be0f177a0db04aeb988c22588c3e95ef",
            "04362094140f43cdac4bfb163aeca3b5",
            "7a4b8ea2a13443dc8775ab76b9785716",
            "fe372d22b3ab46138e676da79a593da3",
            "2ba45990a8124412b8bae4d5dca59130",
            "91251c4ec6564aa7ab1b3536c5165840",
            "72e00893957e4aa6a9aa33469d8b7f29",
            "8bb0b32f5e5f449b863abf20da95ecf1"
          ]
        },
        "id": "EklVbzTfShfX",
        "outputId": "bba316bb-4f1f-4877-c65a-3c3e60b9eb4f",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:43:11.229967Z",
          "iopub.execute_input": "2024-07-08T08:43:11.230787Z",
          "iopub.status.idle": "2024-07-08T08:44:35.506249Z",
          "shell.execute_reply.started": "2024-07-08T08:43:11.230756Z",
          "shell.execute_reply": "2024-07-08T08:44:35.505237Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bb0b32f5e5f449b863abf20da95ecf1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['adapter_config.json', 'README.md', 'pytorch_model-00002-of-00002.bin', 'adapter_model.bin', 'tokenizer.json', 'special_tokens_map.json', 'added_tokens.json', 'tokenizer_config.json', 'tokenizer.model', 'pytorch_model.bin.index.json', 'pytorch_model-00001-of-00002.bin', 'config.json', 'generation_config.json']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Push Model to Hugging Face Hub**"
      ],
      "metadata": {
        "id": "2bn7tjdByJ_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model."
      ],
      "metadata": {
        "id": "CpyMDvc5yQrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Z8VUygTdqVnJ",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:52:11.958847Z",
          "iopub.execute_input": "2024-07-08T08:52:11.959238Z",
          "iopub.status.idle": "2024-07-08T08:52:11.963619Z",
          "shell.execute_reply.started": "2024-07-08T08:52:11.959207Z",
          "shell.execute_reply": "2024-07-08T08:52:11.962716Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, whoami\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "import os\n",
        "\n",
        "# Step 1: Retrieve the Hugging Face token from Kaggle Secrets\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"chatbot\")\n",
        "\n",
        "# Step 2: Login using the Hugging Face token\n",
        "login(token=hf_token)\n",
        "\n",
        "\n",
        "# Step 3: Push the model and tokenizer to the Hugging Face Hub\n",
        "model_repo_name = \"tito92/finance_finetune_model\"\n",
        "tokenizer_repo_name = \"tito92/finance_finetune_model\"\n",
        "\n",
        "# Push the model to the hub\n",
        "model.push_to_hub(model_repo_name, use_auth_token=hf_token, check_pr=True)\n",
        "\n",
        "# Push the tokenizer to the hub\n",
        "tokenizer.push_to_hub(tokenizer_repo_name, use_auth_token=hf_token, check_pr=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T08:52:20.916375Z",
          "iopub.execute_input": "2024-07-08T08:52:20.917065Z",
          "iopub.status.idle": "2024-07-08T08:58:01.131950Z",
          "shell.execute_reply.started": "2024-07-08T08:52:20.917032Z",
          "shell.execute_reply": "2024-07-08T08:58:01.131037Z"
        },
        "trusted": true,
        "id": "9LI6yDie-YHa",
        "outputId": "db6fddf4-09dc-4e65-8a8f-97880e277bad",
        "colab": {
          "referenced_widgets": [
            "3fe490fdfbba48619caef236b9236ec5",
            "1f7291e7b2b54fdc9213ea894fb4ca0e",
            "50c46167e6844dd69c92902f54e62ceb"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fe490fdfbba48619caef236b9236ec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f7291e7b2b54fdc9213ea894fb4ca0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50c46167e6844dd69c92902f54e62ceb"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/tito92/finance_finetune_model/commit/b3e1572b6b3f267b09be970ef8bfb91300cf812f', commit_message='Upload tokenizer', commit_description='', oid='b3e1572b6b3f267b09be970ef8bfb91300cf812f', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now use this model for inference by loading it like any other Llama 2 model from the Hub."
      ],
      "metadata": {
        "id": "6nxb3tkLyXeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T08:58:57.954529Z",
          "iopub.execute_input": "2024-07-08T08:58:57.954906Z",
          "iopub.status.idle": "2024-07-08T08:58:57.987687Z",
          "shell.execute_reply.started": "2024-07-08T08:58:57.954873Z",
          "shell.execute_reply": "2024-07-08T08:58:57.986717Z"
        },
        "trusted": true,
        "id": "UBrtMDrt-YHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the finetuned model from Hugging face Hub\n",
        "fine_tuned_finance_model=AutoModelForCausalLM.from_pretrained('tito92/finance_finetune_model')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T09:03:52.540129Z",
          "iopub.execute_input": "2024-07-08T09:03:52.540888Z",
          "iopub.status.idle": "2024-07-08T09:03:52.886117Z",
          "shell.execute_reply.started": "2024-07-08T09:03:52.540845Z",
          "shell.execute_reply": "2024-07-08T09:03:52.884870Z"
        },
        "trusted": true,
        "id": "_W00d8Kq-YHb",
        "outputId": "b29e44a2-65f9-4f7d-cf04-72d47a0db188"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load the finetuned model from Hugging face Hub\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fine_tuned_finance_model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtito92/finance_finetune_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained('tito92/finance_finetune_model', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "SjhxhwrE-YHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source (output) directory and the new target directory\n",
        "source_dir = \"/kaggle/working/output_dir\"\n",
        "target_dir = \"/kaggle/working/new_dir\"\n",
        "\n",
        "# Ensure the target directory exists, create it if it does not\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Specify the files you want to copy (modify this list as needed)\n",
        "files_to_copy = [\"file1.txt\", \"file2.txt\", \"model.pth\", \"config.json\"]\n",
        "\n",
        "# Copy specified files from source directory to target directory\n",
        "for file_name in files_to_copy:\n",
        "    source_file_path = os.path.join(source_dir, file_name)\n",
        "    target_file_path = os.path.join(target_dir, file_name)\n",
        "    if os.path.exists(source_file_path):\n",
        "        shutil.copy2(source_file_path, target_file_path)\n",
        "        print(f\"Copied {file_name} to {target_dir}\")\n",
        "    else:\n",
        "        print(f\"{file_name} not found in {source_dir}\")\n",
        "\n",
        "print(\"File copying complete.\")\n"
      ],
      "metadata": {
        "id": "7M_rAUlt-YHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}